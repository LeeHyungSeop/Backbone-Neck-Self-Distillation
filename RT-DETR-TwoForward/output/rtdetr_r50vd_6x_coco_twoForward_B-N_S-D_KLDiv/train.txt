WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Initialized distributed mode...
Start training
Load PResNet50 state_dict
weight_dict: {'loss_vfl': 1, 'loss_bbox': 5, 'loss_giou': 2, 'loss_nb_kd': 1}
Initial lr:  [1e-05, 0.0001, 0.0001, 0.0001]
loading annotations into memory...
Done (t=11.25s)
creating index...
index created!
loading annotations into memory...
Done (t=0.37s)
creating index...
index created!
number of params: 42862860
(in det_engine.py) woNeck : False
[det_engine.py > train_one_epoch()] here
[with Neck]
	wNeck : True
Traceback (most recent call last):
  File "/home/hslee/Desktop/Backbone-Neck_Self-Distillation/RT-DETR-TwoForward/tools/train.py", line 50, in <module>
Traceback (most recent call last):
    main(args)
  File "/home/hslee/Desktop/Backbone-Neck_Self-Distillation/RT-DETR-TwoForward/tools/train.py", line 50, in <module>
  File "/home/hslee/Desktop/Backbone-Neck_Self-Distillation/RT-DETR-TwoForward/tools/train.py", line 36, in main
    solver.fit()
  File "/home/hslee/Desktop/Backbone-Neck_Self-Distillation/RT-DETR-TwoForward/tools/../src/solver/det_solver.py", line 38, in fit
    main(args)
    train_stats = train_one_epoch(
                  ^^^^^^^  File "/home/hslee/Desktop/Backbone-Neck_Self-Distillation/RT-DETR-TwoForward/tools/train.py", line 36, in main
^^^^^^^^^
  File "/home/hslee/Desktop/Backbone-Neck_Self-Distillation/RT-DETR-TwoForward/tools/../src/solver/det_engine.py", line 66, in train_one_epoch
    solver.fit()
    outputs_w_neck = model(samples, targets, True)
  File "/home/hslee/Desktop/Backbone-Neck_Self-Distillation/RT-DETR-TwoForward/tools/../src/solver/det_solver.py", line 38, in fit
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    train_stats = train_one_epoch(
                  ^^^^^^^^^^^^^^^^
  File "/home/hslee/Desktop/Backbone-Neck_Self-Distillation/RT-DETR-TwoForward/tools/../src/solver/det_engine.py", line 66, in train_one_epoch
    outputs_w_neck = model(samples, targets, True)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1173, in forward
    list(_find_tensors(output))
    return forward_call(*args, **kwargs)
         ^^^^^^^^^^^ ^ ^^ ^^ ^^ ^ ^^ 
      File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 81, in _find_tensors
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    return itertools.chain(*map(_find_tensors, obj.values()))
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1173, in forward
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 81, in _find_tensors
    return itertools.chain(*map(_find_tensors, obj.values()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 81, in _find_tensors
    return itertools.chain(*map(_find_tensors, obj.values()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    list(_find_tensors(output))
         ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 81, in _find_tensors
    return itertools.chain(*map(_find_tensors, obj.values()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 81, in _find_tensors
    return itertools.chain(*map(_find_tensors, obj.values()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 81, in _find_tensors
    return itertools.chain(*map(_find_tensors, obj.values()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  [Previous line repeated 986 more times]
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 79, in _find_tensors
    return itertools.chain(*map(_find_tensors, obj))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 81, in _find_tensors
    return itertools.chain(*map(_find_tensors, obj.values()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 70, in _find_tensors
    if RPC_AVAILABLE and isinstance(obj, RRef):
                         ^^^^^^^^^^^^^^^^^^^^^
RecursionError: maximum recursion depth exceeded while calling a Python object
  [Previous line repeated 986 more times]
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 79, in _find_tensors
    return itertools.chain(*map(_find_tensors, obj))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 81, in _find_tensors
    return itertools.chain(*map(_find_tensors, obj.values()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 70, in _find_tensors
    if RPC_AVAILABLE and isinstance(obj, RRef):
                         ^^^^^^^^^^^^^^^^^^^^^
RecursionError: maximum recursion depth exceeded while calling a Python object
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 1280146) of binary: /home/hslee/anaconda3/bin/python
Traceback (most recent call last):
  File "/home/hslee/anaconda3/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hslee/anaconda3/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
tools/train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-07-30_23:52:40
  host      : ada
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 1280147)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-30_23:52:40
  host      : ada
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1280146)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
