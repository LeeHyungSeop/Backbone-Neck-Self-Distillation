{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def loss(S, T, gamma = 1):\n",
    "    N = S.shape[0]  # Batch size\n",
    "    loss = (gamma / N) * torch.sum((S - T) ** 2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def total_loss(S_s, T_s, S_m, T_m, S_l, T_l, gamma):\n",
    "    L_s = loss(S_s, T_s, gamma)\n",
    "    L_m = loss(S_m, T_m)\n",
    "    L_l = loss(S_l, T_l)\n",
    "    return L_s + L_m + L_l\n",
    "\n",
    "\n",
    "# 가상의 모델 출력 (예제)\n",
    "batch_size, H, W, C = 4, 32, 32, 64\n",
    "\n",
    "S_s = torch.randn(batch_size, H, W, C)\n",
    "T_s = torch.randn(batch_size, H, W, C)\n",
    "S_m = torch.randn(batch_size, H, W, C)\n",
    "T_m = torch.randn(batch_size, H, W, C)\n",
    "S_l = torch.randn(batch_size, H, W, C)\n",
    "T_l = torch.randn(batch_size, H, W, C)\n",
    "\n",
    "gamma = 0.05\n",
    "\n",
    "# 손실 계산\n",
    "loss = total_loss(S_s, T_s, S_m, T_m, S_l, T_l, gamma)\n",
    "print(f'Total Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KL divergence between two distributions test\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.functional as F\n",
    "\n",
    "# feature map (bs, h, w, c)\n",
    "input = np.random.rand(4, 512, 20, 20)\n",
    "target = np.random.rand(4, 512, 20, 20)\n",
    "\n",
    "input = torch.softmax(torch.tensor(input), dim=-1)\n",
    "\n",
    "# KL divergence\n",
    "kl_div = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "loss = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5468700528144836\n",
      "0.5468700528144836\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "\n",
    "def js_divergence(p, q):\n",
    "    \"\"\"\n",
    "    Compute the Jensen-Shannon Divergence between two probability distributions.\n",
    "    \"\"\"\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * (F.kl_div(p.log(), m, reduction='batchmean') + F.kl_div(q.log(), m, reduction='batchmean'))\n",
    "\n",
    "def compute_js_distance(tensor1, tensor2):\n",
    "    \"\"\"\n",
    "    Compute the Jensen-Shannon distance between two tensors.\n",
    "    \"\"\"\n",
    "    # Reshape tensors to 2D\n",
    "    tensor1 = tensor1.view(tensor1.size(0), -1)\n",
    "    tensor2 = tensor2.view(tensor2.size(0), -1)\n",
    "\n",
    "    # Convert tensors to probability distributions\n",
    "    p = F.softmax(tensor1, dim=1)\n",
    "    q = F.softmax(tensor2, dim=1)\n",
    "\n",
    "    # Compute the JS divergence\n",
    "    js_div = js_divergence(p, q)\n",
    "\n",
    "    # Convert divergence to distance\n",
    "    js_distance = torch.sqrt(js_div)\n",
    "    return js_distance\n",
    "\n",
    "# Example tensors\n",
    "tensor1 = torch.randn(4, 256, 84, 84)\n",
    "tensor2 = torch.randn(4, 256, 84, 84)\n",
    "\n",
    "\n",
    "# Compute the JS distance\n",
    "js_distance = compute_js_distance(tensor1, tensor2)\n",
    "print(js_distance.item())\n",
    "js_distance = compute_js_distance(tensor2, tensor1)\n",
    "print(js_distance.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "torch.Size([4, 256, 20, 20]) torch.Size([4, 256, 20, 20])\n",
      "torch.Size([4, 20, 20]) torch.Size([4, 20, 20])\n",
      "tensor([[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]]) tensor([[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]])\n",
      "---------------\n",
      "torch.Size([4, 256, 400]) torch.Size([4, 256, 400])\n",
      "torch.Size([4, 400]) torch.Size([4, 400])\n",
      "tensor([[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]) tensor([[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "ta= torch.randn(4, 256, 20, 20)\n",
    "tb = torch.randn(4, 256, 20, 20)\n",
    "\n",
    "print(\"---------------\")\n",
    "\n",
    "t1 = torch.softmax(ta, dim=1)\n",
    "t2 = torch.softmax(tb, dim=1)\n",
    "\n",
    "print(t1.shape, t2.shape)\n",
    "print(t1.sum(dim=1).shape, t2.sum(dim=1).shape)\n",
    "print(t1.sum(dim=1), t2.sum(dim=1)) \n",
    "\n",
    "print(\"---------------\")\n",
    "\n",
    "t3 = ta.view(ta.size(0), ta.size(1), -1)\n",
    "t4 = tb.view(tb.size(0), tb.size(1), -1)\n",
    "t3 = torch.softmax(t3, dim=1)\n",
    "t4 = torch.softmax(t4, dim=1)\n",
    "\n",
    "print(t3.shape, t4.shape)\n",
    "print(t3.sum(dim=1).shape, t4.sum(dim=1).shape)\n",
    "print(t3.sum(dim=1), t4.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_grad_cam'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_grad_cam\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EigenCAM\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_grad_cam\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_cam_on_image, scale_cam_image\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_grad_cam'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "import torch    \n",
    "import cv2\n",
    "import numpy as np\n",
    "import requests\n",
    "import torchvision.transforms as transforms\n",
    "from pytorch_grad_cam import EigenCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, scale_cam_image\n",
    "from PIL import Image\n",
    "\n",
    "COLORS = np.random.uniform(0, 255, size=(80, 3))\n",
    "\n",
    "def parse_detections(results):\n",
    "    detections = results.pandas().xyxy[0]\n",
    "    detections = detections.to_dict()\n",
    "    boxes, colors, names = [], [], []\n",
    "\n",
    "    for i in range(len(detections[\"xmin\"])):\n",
    "        confidence = detections[\"confidence\"][i]\n",
    "        if confidence < 0.2:\n",
    "            continue\n",
    "        xmin = int(detections[\"xmin\"][i])\n",
    "        ymin = int(detections[\"ymin\"][i])\n",
    "        xmax = int(detections[\"xmax\"][i])\n",
    "        ymax = int(detections[\"ymax\"][i])\n",
    "        name = detections[\"name\"][i]\n",
    "        category = int(detections[\"class\"][i])\n",
    "        color = COLORS[category]\n",
    "\n",
    "        boxes.append((xmin, ymin, xmax, ymax))\n",
    "        colors.append(color)\n",
    "        names.append(name)\n",
    "    return boxes, colors, names\n",
    "\n",
    "\n",
    "def draw_detections(boxes, colors, names, img):\n",
    "    for box, color, name in zip(boxes, colors, names):\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        cv2.rectangle(\n",
    "            img,\n",
    "            (xmin, ymin),\n",
    "            (xmax, ymax),\n",
    "            color, \n",
    "            2)\n",
    "\n",
    "        cv2.putText(img, name, (xmin, ymin - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2,\n",
    "                    lineType=cv2.LINE_AA)\n",
    "    return img\n",
    "\n",
    "\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/f/f1/Puppies_%284984818141%29.jpg\"\n",
    "img = np.array(Image.open(requests.get(image_url, stream=True).raw))\n",
    "img = cv2.resize(img, (640, 640))\n",
    "rgb_img = img.copy()\n",
    "img = np.float32(img) / 255\n",
    "transform = transforms.ToTensor()\n",
    "tensor = transform(img).unsqueeze(0)\n",
    "\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "model.eval()\n",
    "model.cpu()\n",
    "target_layers = [model.model.model.model[-2]]\n",
    "\n",
    "results = model([rgb_img])\n",
    "boxes, colors, names = parse_detections(results)\n",
    "detections = draw_detections(boxes, colors, names, rgb_img.copy())\n",
    "Image.fromarray(detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
