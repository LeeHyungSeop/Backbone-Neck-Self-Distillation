WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Initialized distributed mode...
Start training
Load PResNet50 state_dict
weight_dict: {'loss_vfl': 1, 'loss_bbox': 5, 'loss_giou': 2}
Initial lr:  [1e-05, 0.0001, 0.0001, 0.0001]
loading annotations into memory...
Done (t=8.13s)
creating index...
index created!
loading annotations into memory...
Done (t=0.22s)
creating index...
index created!
number of params: 43782668
Epoch: [0]  [    0/14786]  eta: 5:37:01  lr: 0.000010  loss: 48.1509 (48.1509)  KD: 3399.3687 (3399.3687)  loss_w_neck: 44.5215 (44.5215)  loss_bbox: 1.5813 (1.5983)  loss_bbox_aux_0: 1.5824 (1.6057)  loss_bbox_aux_1: 1.6079 (1.6144)  loss_bbox_aux_2: 1.5612 (1.5853)  loss_bbox_aux_3: 1.5529 (1.5893)  loss_bbox_aux_4: 1.5666 (1.5932)  loss_bbox_aux_5: 1.6121 (1.6432)  loss_bbox_dn_0: 0.9761 (0.9961)  loss_bbox_dn_1: 0.9761 (0.9961)  loss_bbox_dn_2: 0.9761 (0.9961)  loss_bbox_dn_3: 0.9761 (0.9961)  loss_bbox_dn_4: 0.9761 (0.9961)  loss_bbox_dn_5: 0.9761 (0.9961)  loss_giou: 1.5428 (1.6372)  loss_giou_aux_0: 1.5860 (1.6628)  loss_giou_aux_1: 1.5679 (1.6528)  loss_giou_aux_2: 1.5509 (1.6423)  loss_giou_aux_3: 1.5659 (1.6380)  loss_giou_aux_4: 1.5624 (1.6370)  loss_giou_aux_5: 1.5994 (1.6712)  loss_giou_dn_0: 1.3144 (1.3317)  loss_giou_dn_1: 1.3144 (1.3317)  loss_giou_dn_2: 1.3144 (1.3317)  loss_giou_dn_3: 1.3144 (1.3317)  loss_giou_dn_4: 1.3144 (1.3317)  loss_giou_dn_5: 1.3144 (1.3317)  loss_vfl: 0.3365 (0.4011)  loss_vfl_aux_0: 0.3153 (0.3542)  loss_vfl_aux_1: 0.3489 (0.4014)  loss_vfl_aux_2: 0.3289 (0.3979)  loss_vfl_aux_3: 0.3244 (0.3998)  loss_vfl_aux_4: 0.3432 (0.3961)  loss_vfl_aux_5: 0.3278 (0.3895)  loss_vfl_dn_0: 0.8103 (0.8206)  loss_vfl_dn_1: 0.8834 (0.8886)  loss_vfl_dn_2: 0.8775 (0.8809)  loss_vfl_dn_3: 0.8317 (0.8726)  loss_vfl_dn_4: 0.8847 (0.8922)  loss_vfl_dn_5: 0.9008 (0.9191)  loss_wo_neck: 44.9815 (44.9815)  time: 1.3676  data: 0.4352  max mem: 5501
Epoch: [0]  [  100/14786]  eta: 1:16:19  lr: 0.000010  loss: 38.3889 (42.8353)  KD: 2145.6997 (2851.0824)  loss_w_neck: 36.0917 (39.7288)  loss_bbox: 0.7674 (1.1848)  loss_bbox_aux_0: 0.8190 (1.2697)  loss_bbox_aux_1: 0.7943 (1.2357)  loss_bbox_aux_2: 0.7822 (1.2168)  loss_bbox_aux_3: 0.7754 (1.2046)  loss_bbox_aux_4: 0.7723 (1.1916)  loss_bbox_aux_5: 0.8717 (1.3291)  loss_bbox_dn_0: 0.5989 (0.8637)  loss_bbox_dn_1: 0.6010 (0.8736)  loss_bbox_dn_2: 0.6055 (0.8821)  loss_bbox_dn_3: 0.6078 (0.8897)  loss_bbox_dn_4: 0.6085 (0.8984)  loss_bbox_dn_5: 0.6101 (0.9052)  loss_giou: 1.4629 (1.4805)  loss_giou_aux_0: 1.4893 (1.5234)  loss_giou_aux_1: 1.4779 (1.5032)  loss_giou_aux_2: 1.4533 (1.4948)  loss_giou_aux_3: 1.4596 (1.4889)  loss_giou_aux_4: 1.4620 (1.4838)  loss_giou_aux_5: 1.5283 (1.5546)  loss_giou_dn_0: 1.3373 (1.3364)  loss_giou_dn_1: 1.3361 (1.3361)  loss_giou_dn_2: 1.3401 (1.3389)  loss_giou_dn_3: 1.3445 (1.3426)  loss_giou_dn_4: 1.3501 (1.3483)  loss_giou_dn_5: 1.3546 (1.3537)  loss_vfl: 0.5553 (0.5817)  loss_vfl_aux_0: 0.5059 (0.5126)  loss_vfl_aux_1: 0.5186 (0.5407)  loss_vfl_aux_2: 0.5269 (0.5598)  loss_vfl_aux_3: 0.5301 (0.5701)  loss_vfl_aux_4: 0.5430 (0.5745)  loss_vfl_aux_5: 0.5075 (0.4897)  loss_vfl_dn_0: 0.5169 (0.6177)  loss_vfl_dn_1: 0.5024 (0.6044)  loss_vfl_dn_2: 0.5092 (0.6007)  loss_vfl_dn_3: 0.5059 (0.6069)  loss_vfl_dn_4: 0.4991 (0.5980)  loss_vfl_dn_5: 0.4948 (0.5975)  loss_wo_neck: 35.2321 (40.2396)  time: 0.2985  data: 0.0067  max mem: 8710
