WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Initialized distributed mode...
Start training
Load PResNet50 state_dict
weight_dict: {'loss_vfl': 1, 'loss_bbox': 5, 'loss_giou': 2}
Initial lr:  [1e-05, 0.0001, 0.0001, 0.0001]
loading annotations into memory...
Done (t=8.12s)
creating index...
index created!
loading annotations into memory...
Done (t=0.20s)
creating index...
index created!
number of params: 42862860
Epoch: [0]  [    0/14786]  eta: 5:59:41  lr: 0.000010  loss: 49.6378 (49.6378)  KD: 2583.3350 (2583.3350)  loss_w_neck: 46.9818 (46.9818)  loss_bbox: 1.8118 (1.8290)  loss_bbox_aux_0: 1.8113 (1.8315)  loss_bbox_aux_1: 1.8093 (1.8166)  loss_bbox_aux_2: 1.8192 (1.8268)  loss_bbox_aux_3: 1.8283 (1.8303)  loss_bbox_aux_4: 1.8190 (1.8246)  loss_bbox_aux_5: 1.8504 (1.8698)  loss_bbox_dn_0: 1.0679 (1.0766)  loss_bbox_dn_1: 1.0679 (1.0766)  loss_bbox_dn_2: 1.0679 (1.0766)  loss_bbox_dn_3: 1.0679 (1.0766)  loss_bbox_dn_4: 1.0679 (1.0766)  loss_bbox_dn_5: 1.0679 (1.0766)  loss_giou: 1.7019 (1.7042)  loss_giou_aux_0: 1.7310 (1.7458)  loss_giou_aux_1: 1.7346 (1.7390)  loss_giou_aux_2: 1.7038 (1.7205)  loss_giou_aux_3: 1.7009 (1.7180)  loss_giou_aux_4: 1.6985 (1.7149)  loss_giou_aux_5: 1.7276 (1.7349)  loss_giou_dn_0: 1.3301 (1.3397)  loss_giou_dn_1: 1.3301 (1.3397)  loss_giou_dn_2: 1.3301 (1.3397)  loss_giou_dn_3: 1.3301 (1.3397)  loss_giou_dn_4: 1.3301 (1.3397)  loss_giou_dn_5: 1.3301 (1.3397)  loss_vfl: 0.3386 (0.3591)  loss_vfl_aux_0: 0.3206 (0.3325)  loss_vfl_aux_1: 0.3267 (0.3602)  loss_vfl_aux_2: 0.3379 (0.3524)  loss_vfl_aux_3: 0.3499 (0.3699)  loss_vfl_aux_4: 0.3554 (0.3645)  loss_vfl_aux_5: 0.3561 (0.3563)  loss_vfl_dn_0: 0.8544 (0.8707)  loss_vfl_dn_1: 0.8469 (0.8688)  loss_vfl_dn_2: 0.8316 (0.8471)  loss_vfl_dn_3: 0.8161 (0.8327)  loss_vfl_dn_4: 0.8371 (0.8640)  loss_vfl_dn_5: 0.8521 (0.8726)  loss_wo_neck: 47.1272 (47.1272)  time: 1.4596  data: 0.4419  max mem: 7353
Epoch: [0]  [  100/14786]  eta: 1:16:55  lr: 0.000010  loss: 38.2736 (41.5555)  KD: 2308.6648 (2112.3067)  loss_w_neck: 35.5278 (39.4282)  loss_bbox: 0.7447 (1.1296)  loss_bbox_aux_0: 0.8029 (1.1897)  loss_bbox_aux_1: 0.7848 (1.1658)  loss_bbox_aux_2: 0.7612 (1.1503)  loss_bbox_aux_3: 0.7512 (1.1414)  loss_bbox_aux_4: 0.7442 (1.1340)  loss_bbox_aux_5: 0.8717 (1.2341)  loss_bbox_dn_0: 0.6866 (0.8668)  loss_bbox_dn_1: 0.7065 (0.8747)  loss_bbox_dn_2: 0.7177 (0.8819)  loss_bbox_dn_3: 0.7189 (0.8875)  loss_bbox_dn_4: 0.7174 (0.8926)  loss_bbox_dn_5: 0.7161 (0.8973)  loss_giou: 1.3403 (1.4419)  loss_giou_aux_0: 1.3828 (1.4776)  loss_giou_aux_1: 1.3564 (1.4598)  loss_giou_aux_2: 1.3486 (1.4535)  loss_giou_aux_3: 1.3476 (1.4487)  loss_giou_aux_4: 1.3425 (1.4455)  loss_giou_aux_5: 1.4058 (1.5013)  loss_giou_dn_0: 1.3442 (1.3379)  loss_giou_dn_1: 1.3424 (1.3386)  loss_giou_dn_2: 1.3465 (1.3415)  loss_giou_dn_3: 1.3493 (1.3456)  loss_giou_dn_4: 1.3587 (1.3498)  loss_giou_dn_5: 1.3621 (1.3551)  loss_vfl: 0.6457 (0.6116)  loss_vfl_aux_0: 0.6352 (0.5613)  loss_vfl_aux_1: 0.6344 (0.5798)  loss_vfl_aux_2: 0.6391 (0.5967)  loss_vfl_aux_3: 0.6282 (0.6066)  loss_vfl_aux_4: 0.6205 (0.6095)  loss_vfl_aux_5: 0.6138 (0.5496)  loss_vfl_dn_0: 0.4987 (0.6080)  loss_vfl_dn_1: 0.4954 (0.6022)  loss_vfl_dn_2: 0.4938 (0.5894)  loss_vfl_dn_3: 0.4940 (0.5959)  loss_vfl_dn_4: 0.5019 (0.5998)  loss_vfl_dn_5: 0.4990 (0.5905)  loss_wo_neck: 35.5162 (39.4581)  time: 0.3016  data: 0.0065  max mem: 8817
Epoch: [0]  [  200/14786]  eta: 1:15:03  lr: 0.000010  loss: 36.8187 (39.9036)  KD: 1757.6240 (2099.3998)  loss_w_neck: 35.0908 (37.9049)  loss_bbox: 0.6841 (0.9327)  loss_bbox_aux_0: 0.7978 (1.0113)  loss_bbox_aux_1: 0.7470 (0.9758)  loss_bbox_aux_2: 0.7222 (0.9563)  loss_bbox_aux_3: 0.7038 (0.9458)  loss_bbox_aux_4: 0.6919 (0.9374)  loss_bbox_aux_5: 0.8734 (1.0628)  loss_bbox_dn_0: 0.9242 (0.8644)  loss_bbox_dn_1: 0.9187 (0.8651)  loss_bbox_dn_2: 0.9189 (0.8676)  loss_bbox_dn_3: 0.9229 (0.8696)  loss_bbox_dn_4: 0.9252 (0.8718)  loss_bbox_dn_5: 0.9273 (0.8741)  loss_giou: 1.1830 (1.3570)  loss_giou_aux_0: 1.2492 (1.4089)  loss_giou_aux_1: 1.2024 (1.3839)  loss_giou_aux_2: 1.1932 (1.3727)  loss_giou_aux_3: 1.1786 (1.3645)  loss_giou_aux_4: 1.1658 (1.3601)  loss_giou_aux_5: 1.3332 (1.4429)  loss_giou_dn_0: 1.2804 (1.3301)  loss_giou_dn_1: 1.2635 (1.3280)  loss_giou_dn_2: 1.2578 (1.3325)  loss_giou_dn_3: 1.2582 (1.3373)  loss_giou_dn_4: 1.2609 (1.3427)  loss_giou_dn_5: 1.2618 (1.3487)  loss_vfl: 0.9034 (0.7230)  loss_vfl_aux_0: 0.8256 (0.6457)  loss_vfl_aux_1: 0.8273 (0.6688)  loss_vfl_aux_2: 0.8481 (0.6933)  loss_vfl_aux_3: 0.8688 (0.7035)  loss_vfl_aux_4: 0.8875 (0.7133)  loss_vfl_aux_5: 0.7325 (0.6277)  loss_vfl_dn_0: 0.4665 (0.5496)  loss_vfl_dn_1: 0.4854 (0.5503)  loss_vfl_dn_2: 0.4905 (0.5422)  loss_vfl_dn_3: 0.5203 (0.5477)  loss_vfl_dn_4: 0.5112 (0.5509)  loss_vfl_dn_5: 0.5222 (0.5439)  loss_wo_neck: 34.7516 (37.7034)  time: 0.2970  data: 0.0067  max mem: 8973
