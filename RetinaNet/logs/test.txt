/home/hslee/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
| distributed init (rank 0): env://
Namespace(data_path='/home/hslee/Desktop/Datasets/coco', dataset='coco', model='retinanet_resnet50_fpn', device='cuda', batch_size=16, epochs=26, workers=4, opt='sgd', lr=0.0025, momentum=0.9, weight_decay=0.0001, norm_weight_decay=None, lr_scheduler='multisteplr', lr_step_size=8, lr_steps=[16, 22], lr_gamma=0.1, print_freq=20, output_dir='.', resume='', start_epoch=0, aspect_ratio_group_factor=3, rpn_score_thresh=None, trainable_backbone_layers=None, data_augmentation='hflip', sync_bn=False, test_only=False, use_deterministic_algorithms=False, world_size=1, dist_url='env://', weights=None, weights_backbone='ResNet50_Weights.IMAGENET1K_V1', amp=False, use_copypaste=False, backend='pil', use_v2=False, rank=0, gpu=0, distributed=True, dist_backend='nccl')
Loading data
loading annotations into memory...
Done (t=7.63s)
creating index...
index created!
loading annotations into memory...
Done (t=0.24s)
creating index...
index created!
Creating data loaders
Using [0, 0.5, 0.6299605249474365, 0.7937005259840997, 1.0, 1.259921049894873, 1.5874010519681991, 2.0, inf] as bins for aspect ratio quantization
Count of instances per bin: [  104   982 24236  2332  8225 74466  5763  1158]
Creating model
Start training
Epoch: [0]  [   0/7329]  eta: 3:48:16  lr: 0.000005  loss: 2.1738 (2.1738)  classification: 1.3626 (1.3626)  bbox_regression: 0.8112 (0.8112)  time: 1.8689  data: 0.5699  max mem: 20518
Epoch: [0]  [  20/7329]  eta: 2:29:11  lr: 0.000055  loss: 1.9769 (2.0158)  classification: 1.2750 (1.2983)  bbox_regression: 0.6955 (0.7174)  time: 1.1926  data: 0.0184  max mem: 22822
Epoch: [0]  [  40/7329]  eta: 2:24:31  lr: 0.000105  loss: 1.9951 (2.0177)  classification: 1.2755 (1.2993)  bbox_regression: 0.6991 (0.7184)  time: 1.1529  data: 0.0186  max mem: 22883
Epoch: [0]  [  60/7329]  eta: 2:22:58  lr: 0.000155  loss: 1.9859 (2.0075)  classification: 1.2827 (1.2937)  bbox_regression: 0.6809 (0.7139)  time: 1.1607  data: 0.0185  max mem: 22883
Traceback (most recent call last):
  File "/home/hslee/Desktop/Embedded_AI/Backbone-Neck-Self-Distillation/RetinaNet/train.py", line 334, in <module>
    main(args)
  File "/home/hslee/Desktop/Embedded_AI/Backbone-Neck-Self-Distillation/RetinaNet/train.py", line 309, in main
    train_one_epoch(model, optimizer, data_loader, device, epoch, args.print_freq, scaler)
  File "/home/hslee/Desktop/Embedded_AI/Backbone-Neck-Self-Distillation/RetinaNet/engine.py", line 31, in train_one_epoch
    loss_dict = model(images, targets)
  File "/home/hslee/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hslee/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/home/hslee/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1110, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]
  File "/home/hslee/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hslee/anaconda3/envs/yolov10/lib/python3.9/site-packages/torchvision/models/detection/retinanet.py", line 645, in forward
    losses = self.compute_loss(targets, head_outputs, anchors)
  File "/home/hslee/anaconda3/envs/yolov10/lib/python3.9/site-packages/torchvision/models/detection/retinanet.py", line 503, in compute_loss
    return self.head.compute_loss(targets, head_outputs, anchors, matched_idxs)
  File "/home/hslee/anaconda3/envs/yolov10/lib/python3.9/site-packages/torchvision/models/detection/retinanet.py", line 78, in compute_loss
    "classification": self.classification_head.compute_loss(targets, head_outputs, matched_idxs),
  File "/home/hslee/anaconda3/envs/yolov10/lib/python3.9/site-packages/torchvision/models/detection/retinanet.py", line 181, in compute_loss
    sigmoid_focal_loss(
  File "/home/hslee/anaconda3/envs/yolov10/lib/python3.9/site-packages/torchvision/ops/focal_loss.py", line 39, in sigmoid_focal_loss
    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction="none")
  File "/home/hslee/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/nn/functional.py", line 3165, in binary_cross_entropy_with_logits
    return torch.binary_cross_entropy_with_logits(input, target, weight, pos_weight, reduction_enum)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 0; 23.59 GiB total capacity; 21.97 GiB already allocated; 10.69 MiB free; 23.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2332589) of binary: /home/hslee/anaconda3/envs/yolov10/bin/python
Traceback (most recent call last):
  File "/home/hslee/anaconda3/envs/yolov10/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/hslee/anaconda3/envs/yolov10/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/hslee/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/distributed/launch.py", line 196, in <module>
    main()
  File "/home/hslee/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/distributed/launch.py", line 192, in main
    launch(args)
  File "/home/hslee/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/distributed/launch.py", line 177, in launch
    run(args)
  File "/home/hslee/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/hslee/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/hslee/anaconda3/envs/yolov10/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-29_20:31:40
  host      : hslee-desktop
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2332589)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
